# üìò Guia de Uso - Personaliza√ß√µes do Blog

Este documento explica como usar as personaliza√ß√µes implementadas no blog.

## üé® Shortcodes Dispon√≠veis

### 1. Alert Box

Use para destacar informa√ß√µes importantes:

```markdown
{{< alert type="info" >}}
**Apache Spark** usa lazy evaluation por padr√£o. Isso significa que transforma√ß√µes n√£o s√£o executadas imediatamente.
{{< /alert >}}

{{< alert type="warning" >}}
‚ö†Ô∏è **Aten√ß√£o:** Usar `collect()` em DataFrames grandes pode causar **OutOfMemory**.
{{< /alert >}}

{{< alert type="tip" >}}
üí° **Dica:** Use `cache()` apenas quando for reutilizar o DataFrame m√∫ltiplas vezes.
{{< /alert >}}

{{< alert type="spark" >}}
üî• Configura√ß√£o recomendada para jobs Spark em produ√ß√£o...
{{< /alert >}}
```

#### Tipos de Alert
- `info` - Informa√ß√µes gerais (cyan)
- `warning` - Avisos e cuidados (laranja)
- `tip` - Dicas e boas pr√°ticas (verde)
- `danger` - Perigos e erros cr√≠ticos (vermelho)
- `spark` - Espec√≠fico para Apache Spark (laranja Spark)

### 2. Code Snippet com T√≠tulo

Use para blocos de c√≥digo com contexto:

```markdown
{{< code lang="python" title="spark_optimization.py" >}}
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("OptimizedJob") \
    .config("spark.sql.adaptive.enabled", "true") \
    .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
    .getOrCreate()

df = spark.read.parquet("s3://bucket/data/")
result = df.filter(df.status == "active").groupBy("category").count()
{{< /code >}}
```

#### Linguagens Suportadas
- `python`
- `sql`
- `bash`
- `scala`
- `java`
- `yaml`
- `json`
- `toml`

## üéØ Templates de Post

### Criar Novo Post

```bash
~/bin/hugo new posts/otimizacao-spark-shuffle.md
```

O template j√° vem com a estrutura completa:

```markdown
---
title: "Otimiza√ß√£o Spark Shuffle"
date: 2025-10-13T10:00:00-03:00
draft: true
tags: ["spark", "performance", "data-engineering"]
categories: ["tutorial", "performance"]
author: "Claudio Melo"
description: "Como otimizar opera√ß√µes de shuffle no Apache Spark para melhorar performance em at√© 10x"
ShowToc: true
---

## Introdu√ß√£o
## Conceitos Fundamentais
## Solu√ß√£o
## Exemplos Pr√°ticos
## Considera√ß√µes
## Conclus√£o
## Refer√™ncias
```

### Categorias Recomendadas

- `tutorial` - Guias passo a passo
- `conceitos` - Explica√ß√µes te√≥ricas
- `arquitetura` - Design patterns
- `performance` - Otimiza√ß√µes
- `boas-praticas` - Recomenda√ß√µes
- `case-study` - Estudos de caso

### Tags Sugeridas

**Apache Spark:**
- `spark`, `pyspark`, `spark-sql`, `spark-streaming`
- `performance`, `optimization`, `shuffle`, `caching`

**Data Lakes:**
- `data-lake`, `delta-lake`, `iceberg`, `hudi`
- `parquet`, `orc`, `avro`

**Cloud:**
- `aws`, `s3`, `emr`, `glue`
- `azure`, `databricks`
- `gcp`, `bigquery`

**Arquitetura:**
- `data-mesh`, `data-fabric`, `lakehouse`
- `etl`, `elt`, `pipeline`

**Python:**
- `python`, `pandas`, `polars`
- `airflow`, `prefect`

## üé® Customiza√ß√µes CSS

### Cores Principais

```css
--accent-color: #00d9ff;      /* Cyan - cor principal */
--spark-orange: #e25a1c;      /* Apache Spark */
--data-green: #00c853;        /* Sucesso/dados */
--warning-amber: #ffa726;     /* Avisos */
```

### Aplicar Destaque Especial

Para destacar tecnologias espec√≠ficas, use **negrito**:

```markdown
Neste tutorial vamos usar **Apache Spark**, **PySpark** e **Delta Lake**.
```

No CSS customizado, palavras em negrito ficam com a cor accent automaticamente.

## üìù Exemplos de Posts

### Exemplo 1: Tutorial T√©cnico

```markdown
---
title: "Como Otimizar Joins no Apache Spark"
date: 2025-10-13T14:00:00-03:00
draft: false
tags: ["spark", "performance", "joins", "data-engineering"]
categories: ["tutorial", "performance"]
description: "Guia completo de otimiza√ß√£o de joins no Spark com broadcast, bucketing e estat√≠sticas"
ShowToc: true
---

## Introdu√ß√£o

Joins s√£o opera√ß√µes custosas no Spark. Vamos explorar t√©cnicas para otimiz√°-los.

{{< alert type="spark" >}}
üî• **Performance Tip:** Joins podem causar shuffle massivo de dados se n√£o otimizados.
{{< /alert >}}

## Conceitos Fundamentais

### Tipos de Join no Spark

- **Broadcast Join** - Para datasets pequenos (<10MB)
- **Sort-Merge Join** - Join padr√£o para grandes datasets
- **Shuffle Hash Join** - Quando uma tabela √© pequena mas n√£o cabe em broadcast

## Solu√ß√£o

### 1. Broadcast Join

{{< code lang="python" title="broadcast_join.py" >}}
from pyspark.sql.functions import broadcast

# For√ßa broadcast da tabela menor
result = large_df.join(
    broadcast(small_df),
    "key",
    "inner"
)
{{< /code >}}

{{< alert type="tip" >}}
üí° Use broadcast apenas para DataFrames < 10MB em ambiente de produ√ß√£o.
{{< /alert >}}

## Exemplos Pr√°ticos

### Compara√ß√£o de Performance

| T√©cnica | Tempo | Shuffle |
|---------|-------|---------|
| Join Normal | 45s | 2.5GB |
| Broadcast Join | 8s | 0GB |
| Bucketed Join | 12s | 500MB |

## Considera√ß√µes

### Quando Usar Cada T√©cnica

- ‚úÖ **Broadcast:** Tabela < 10MB, join frequente
- ‚úÖ **Bucketing:** Tabelas grandes, joins repetidos
- ‚ö†Ô∏è **Sort-Merge:** Padr√£o, mas pode ser lento

## Conclus√£o

Escolher a estrat√©gia certa de join pode reduzir o tempo em at√© 80%.

## Refer√™ncias

- [Spark SQL Guide - Joins](https://spark.apache.org/docs/latest/sql-performance-tuning.html)
- [Databricks - Join Optimization](https://docs.databricks.com)
```

### Exemplo 2: Conceito Explicado

```markdown
---
title: "Entendendo Particionamento no Apache Spark"
date: 2025-10-13T16:00:00-03:00
draft: false
tags: ["spark", "conceitos", "partitioning"]
categories: ["conceitos"]
description: "Explica√ß√£o detalhada sobre particionamento no Spark e como ele afeta performance"
ShowToc: true
---

## Introdu√ß√£o

Particionamento √© fundamental para paralelizar processamento no Spark.

{{< alert type="info" >}}
‚ÑπÔ∏è **Defini√ß√£o:** Parti√ß√£o √© uma divis√£o l√≥gica dos dados distribu√≠da pelos n√≥s do cluster.
{{< /alert >}}

## Conceitos Fundamentais

### O que √© Particionamento?

Spark divide dados em **parti√ß√µes** que s√£o processadas em paralelo...

[Continue com a explica√ß√£o...]
```

## üöÄ Comandos √öteis

### Desenvolvimento

```bash
# Servidor com drafts e live reload
~/bin/hugo server -D

# Servidor sem drafts (produ√ß√£o)
~/bin/hugo server

# Build para produ√ß√£o
~/bin/hugo --minify
```

### Git

```bash
# Atualizar tema PaperMod
git submodule update --remote --merge

# Commit das mudan√ßas
git add .
git commit -m "feat: novo post sobre Spark"
git push
```

## üìä Checklist de Publica√ß√£o

Antes de publicar um post (`draft: false`):

- [ ] T√≠tulo claro e descritivo
- [ ] Description entre 150-160 caracteres
- [ ] Tags relevantes (3-5 tags)
- [ ] Categoria apropriada
- [ ] Data correta
- [ ] TOC habilitado se necess√°rio
- [ ] C√≥digo com syntax highlighting
- [ ] Exemplos pr√°ticos inclu√≠dos
- [ ] Ortografia revisada
- [ ] Links funcionando
- [ ] Imagens otimizadas (se houver)
- [ ] Refer√™ncias citadas

## üéØ SEO Best Practices

### Title
- M√°ximo 60 caracteres
- Inclua palavra-chave principal
- Seja descritivo

### Description
- Entre 150-160 caracteres
- Inclua call-to-action
- Descreva o valor do conte√∫do

### URLs
- Use slugs amig√°veis
- Evite caracteres especiais
- Mantenha curto e descritivo

Exemplo bom: `/posts/otimizar-spark-joins/`
Exemplo ruim: `/posts/como-otimizar-operacoes-de-join-no-apache-spark-para-melhorar-performance/`

## üì± Testes de Responsividade

Antes de publicar, teste em:

- Desktop (1920x1080)
- Tablet (768x1024)
- Mobile (375x667)

Use as DevTools do navegador para testar.

---

**D√∫vidas?** Consulte a documenta√ß√£o oficial do [Hugo](https://gohugo.io/documentation/) e [PaperMod](https://github.com/adityatelange/hugo-PaperMod/wiki).
